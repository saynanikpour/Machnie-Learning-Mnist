import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist

# بارگیری مجموعه داده MNIST و گزارش ابعاد داده‌های آموزش و آزمون
(x_train, y_train), (x_test, y_test) = mnist.load_data()

print("Dimensions of training data:")
print("Number of samples:", x_train.shape[0])
print("Dimensions of each image:", x_train.shape[1:])

print("\nDimensions of testing data:")
print("Number of samples:", x_test.shape[0])
print("Dimensions of each image:", x_test.shape[1:])

# نمایش یک نمونه از هر کلاس
classes = np.unique(y_train)
plt.figure(figsize=(10, 5))
for i, cls in enumerate(classes):
    sample_image = x_train[y_train == cls][0]
    plt.subplot(2, 5, i+1)
    plt.imshow(sample_image, cmap='gray')
    plt.title(f"Class: {cls}")
    plt.axis('off')
plt.show()

# رسم نمودار هیستوگرام برای تعداد نمونه‌های هر کلاس در داده‌های آموزش و آزمون
plt.figure(figsize=(10, 5))
plt.hist(y_test, bins=len(classes), alpha=0.5, label='Test', color='blue')
plt.hist(y_train, bins=len(classes), alpha=0.5, label='Train', color='green')
plt.title('Histogram of Class Distribution')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.legend()
plt.show()

# اسکیل کردن داده‌ها به بازه [0, 1]
x_train = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))
x_test = (x_test - np.min(x_test)) / (np.max(x_test) - np.min(x_test))

# یا می‌توانید از x_train /= 255.0 و x_test /= 255.0 به جای محاسبه دوباره استفاده کنید.
# x_train /= 255.0
# x_test /= 255.0

# تبدیل برچسب‌ها به کدگذاری one-hot
y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)

# تعریف مدل MLP
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(120, activation='relu'),
    tf.keras.layers.Dense(84, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# کامپایل مدل
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.GradientTape() as tape:
    predictions = model(x_train[:100])  # استفاده از تنها یک زیرمجموعه برای سرعت
    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_train_onehot[:100], predictions))

gradients = tape.gradient(loss, model.trainable_variables)
total_gradients = [tf.reduce_sum(gradient).numpy() for gradient in gradients]
total_gradients = np.sum(total_gradients)
print("Number of weight updates per epoch:",total_gradients)

# محاسبه تعداد به‌روزرسانی وزن در طی 5 ایپاک
num_batches = len(x_train) // 200
num_epochs = 5
updates_per_epoch = num_batches // num_epochs
print("Number of weight updates per epoch", updates_per_epoch)

# آموزش مدل و به‌روزرسانی وزن‌ها
for epoch in range(num_epochs):
    print("epoch:", epoch + 1)
    for batch in range(num_batches):
        # محاسبه هزینه
        x_batch = x_train[batch * 200:(batch + 1) * 200]
        y_batch = y_train_onehot[batch * 200:(batch + 1) * 200]
        loss = model.train_on_batch(x_batch, y_batch)

        # به‌روزرسانی وزن‌ها از طریق backpropagation
        with tf.GradientTape() as tape:
            predictions = model(x_batch)
            loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_batch, predictions))
        gradients = tape.gradient(loss, model.trainable_variables)
        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    # محاسبه دقت و هزینه آزمون
    test_loss, test_accuracy = model.evaluate(x_test, tf.keras.utils.to_categorical(y_test, 10))
    print("Test loss:", test_loss)
    print("Test accuracy:", test_accuracy)
